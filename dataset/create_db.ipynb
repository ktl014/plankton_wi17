{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, shutil, json, csv\n",
    "import numpy as np\n",
    "import skimage.io as sio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "001/132: 20170124_001\n",
      "002/132: 20170124_002\n",
      "003/132: 20170126_001\n",
      "004/132: 20170126_002\n",
      "005/132: 20170126_003\n",
      "006/132: 20170130_001\n",
      "007/132: 20170130_002\n",
      "008/132: 20170130_003\n",
      "009/132: 20170131_001\n",
      "010/132: 20170131_002\n",
      "011/132: 20170203_001\n",
      "012/132: 20170203_002\n",
      "013/132: 20170206_001\n",
      "014/132: 20170206_002\n",
      "015/132: 20170207_001\n",
      "016/132: 20170207_003\n",
      "017/132: 20170207_004\n",
      "018/132: 20170207_006\n",
      "019/132: 20170209_001\n",
      "020/132: 20170209_002\n",
      "021/132: 20170209_003\n",
      "022/132: 20170210_001\n",
      "023/132: 20170210_002\n",
      "024/132: 20170213_001\n",
      "025/132: 20170213_002\n",
      "026/132: 20170213_003\n",
      "027/132: 20170214_001\n",
      "028/132: 20170214_002\n",
      "029/132: 20170214_003\n",
      "030/132: 20170214_004\n",
      "031/132: 20170214_005\n",
      "032/132: 20170214_006\n",
      "033/132: 20170216_001\n",
      "034/132: 20170216_002\n",
      "035/132: 20170216_003\n",
      "036/132: 20170216_004\n",
      "037/132: 20170216_005\n",
      "038/132: 20170216_006\n",
      "039/132: 20170217_001\n",
      "040/132: 20170217_002\n",
      "041/132: 20170217_003\n",
      "042/132: 20170217_004\n",
      "043/132: 20170217_005\n",
      "044/132: 20170217_006\n",
      "045/132: 20170217_007\n",
      "046/132: 20170221_001\n",
      "047/132: 20170221_002\n",
      "048/132: 20170221_003\n",
      "049/132: 20170221_004\n",
      "050/132: 20170221_005\n",
      "051/132: 20170221_006\n",
      "052/132: 20170221_007\n",
      "053/132: 20170223_001\n",
      "054/132: 20170223_002\n",
      "055/132: 20170224_001\n",
      "056/132: 20170224_002\n",
      "057/132: 20170224_003\n",
      "058/132: 20170224_004\n",
      "059/132: 20170224_005\n",
      "060/132: 20170224_006\n",
      "061/132: 20170224_007\n",
      "062/132: 20170224_008\n",
      "063/132: 20170224_009\n",
      "064/132: 20170224_010\n",
      "065/132: 20170224_011\n",
      "066/132: 20170224_012\n",
      "067/132: 20170228_001\n",
      "068/132: 20170228_002\n",
      "069/132: 20170228_003\n",
      "070/132: 20170302_001\n",
      "071/132: 20170302_002\n",
      "072/132: 20170303_001\n",
      "073/132: 20170303_002\n",
      "074/132: 20170303_003\n",
      "075/132: 20170303_004\n",
      "076/132: 20170303_005\n",
      "077/132: 20170306_001\n",
      "078/132: 20170306_002\n",
      "079/132: 20170306_003\n",
      "080/132: 20170306_004\n",
      "081/132: 20170306_005\n",
      "082/132: 20170306_006\n",
      "083/132: 20170306_007\n",
      "084/132: 20170307_001\n",
      "085/132: 20170307_002\n",
      "086/132: 20170307_003\n",
      "087/132: 20170420_001\n",
      "088/132: 20170425_001\n",
      "089/132: 20170425_002\n",
      "090/132: 20170425_003\n",
      "091/132: 20170425_004\n",
      "092/132: 20170425_005\n",
      "093/132: 20170427_001\n",
      "094/132: 20170427_002\n",
      "095/132: 20170427_003\n",
      "096/132: 20170427_004\n",
      "097/132: 20170502_001\n",
      "098/132: 20170502_002\n",
      "099/132: 20170504_001\n",
      "100/132: 20170504_002\n",
      "101/132: 20170504_003\n",
      "102/132: 20170504_004\n",
      "103/132: 20170504_005\n",
      "104/132: 20170504_006\n",
      "105/132: 20170504_007\n",
      "106/132: 20170509_001\n",
      "107/132: 20170509_002\n",
      "108/132: 20170509_003\n",
      "109/132: 20170509_004\n",
      "110/132: 20170509_005\n",
      "111/132: 20170516_001\n",
      "112/132: 20170516_002\n",
      "113/132: 20170516_003\n",
      "114/132: 20170516_004\n",
      "115/132: 20170516_005\n",
      "116/132: 20170516_006\n",
      "117/132: 20170516_007\n",
      "118/132: 20170516_008\n",
      "119/132: 20170518_001\n",
      "120/132: 20170601_001\n",
      "121/132: 20170601_002\n",
      "122/132: 20170601_003\n",
      "123/132: 20171128_001\n",
      "124/132: 20171130_001\n",
      "125/132: 20171130_004\n",
      "126/132: 20171205_001\n",
      "127/132: 20171207_001\n",
      "128/132: 20171207_002\n",
      "129/132: 20171207_003\n",
      "130/132: 20171207_004\n",
      "131/132: 20171207_005\n",
      "132/132: 20171207_006\n"
     ]
    }
   ],
   "source": [
    "inp_dir = '/data4/plankton_wi17/plankton/images_orig'\n",
    "out_dir = 'rawcolor_db/images'\n",
    "specimen_list = []\n",
    "image_list = {}\n",
    "for session in sorted(os.listdir(inp_dir)):\n",
    "    session_dir = os.path.join(inp_dir, session)\n",
    "    if not os.path.isdir(session_dir):\n",
    "        continue\n",
    "    for specimen in sorted(os.listdir(session_dir)):\n",
    "        specimen_dir = os.path.join(session_dir, specimen)\n",
    "        if not os.path.isdir(specimen_dir):\n",
    "            continue\n",
    "        images = glob.glob(specimen_dir+'/0000000_static_html/images/*/*_rawcolor.png')\n",
    "        \n",
    "        images2 = {}\n",
    "        for fn in images:\n",
    "            iid = '_'.join(fn.split('/')[-1].split('-')[1:3])\n",
    "            if iid not in images2:\n",
    "                images2[iid] = []\n",
    "            images2[iid].append(fn)\n",
    "        \n",
    "        images3 = []\n",
    "        for iid in images2:\n",
    "            if len(images2[iid]) == 1:\n",
    "                images3.append(images2[iid][0])\n",
    "                \n",
    "        if len(images3) < 200:\n",
    "            continue\n",
    "        \n",
    "        spc = '{}_{}'.format(session, specimen)\n",
    "        specimen_list.append(spc)\n",
    "        image_list[spc] = images3\n",
    "        \n",
    "\n",
    "if not os.path.isdir(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "for i, spc in enumerate(specimen_list):\n",
    "    print '{:03d}/{:03d}: {}'.format(i+1, len(specimen_list), spc)\n",
    "    spc_dir = os.path.join(out_dir, spc)\n",
    "    if not os.path.isdir(spc_dir):\n",
    "        os.makedirs(spc_dir)\n",
    "    for src in image_list[spc]:\n",
    "        dest = os.path.join(spc_dir, src.split('/')[-1])\n",
    "        shutil.copyfile(src, dest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "001/132: 20170124_001\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'rawcolor_db/meta/20170124_001-meta.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-99c80da603ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;31m# Convert database to json\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mconvert_taffy2json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;31m# Add filenames and labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-99c80da603ed>\u001b[0m in \u001b[0;36mconvert_taffy2json\u001b[0;34m(inp_fn, out_fn)\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mentry_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m# Load taxonomy labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'rawcolor_db/meta/20170124_001-meta.json'"
     ]
    }
   ],
   "source": [
    "INP_DIR = '/data4/plankton_wi17/plankton/images_orig/'\n",
    "OUT_DIR = 'rawcolor_db/meta'\n",
    "def convert_taffy2json(inp_fn, out_fn):\n",
    "    # I looked for some 3rd-party reader for taffy but could not find any.\n",
    "    # This probably needs to be improved, if we start getting errors.\n",
    "    # However, it is working for now.\n",
    "    import json\n",
    "    import re\n",
    "    database = ''.join([l for l in open(inp_fn)])\n",
    "    database = re.split('TAFFY\\(|\\)', database)[1].splitlines()\n",
    "    \n",
    "    head = 0\n",
    "    entry_list = []\n",
    "    while head < len(database) - 1:\n",
    "        head += 1\n",
    "        line = database[head].strip()\n",
    "        if line.startswith('{'):\n",
    "            entry = {}\n",
    "            while True:\n",
    "                head += 1\n",
    "                line = database[head].strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                if line.startswith('}'):\n",
    "                    break\n",
    "                spt = line.split(':')\n",
    "                key = spt[0]\n",
    "                val = ''.join(spt[1:]).strip().split(',')[0]\n",
    "                if val.startswith(\"'\"):\n",
    "                    val = val[1:-1]\n",
    "                else:\n",
    "                    val = float(val)\n",
    "                entry[key] = val\n",
    "            entry_list.append(entry)\n",
    "    json.dump(entry_list, open(out_fn, 'w'))\n",
    "\n",
    "# Load taxonomy labels\n",
    "taxonomy = open('specimen_taxonomy.txt').read().splitlines()\n",
    "taxonomy = [entry.split('\\t') for entry in taxonomy[1:]]\n",
    "order = {e[0]: e[1] for e in taxonomy}\n",
    "family = {e[0]: e[2] for e in taxonomy}\n",
    "genus = {e[0]: e[3] for e in taxonomy}\n",
    "\n",
    "specimen_list = sorted(os.listdir('rawcolor_db/images'))\n",
    "for i, spc in enumerate(specimen_list):\n",
    "    print '{:03d}/{:03d}: {}'.format(i+1, len(specimen_list), spc)\n",
    "    session, series = spc.split('_')\n",
    "    out_fn = os.path.join(OUT_DIR, '{}-meta.json'.format(spc))\n",
    "    inp_fn = os.path.join(INP_DIR, session, series, '0000000_static_html', 'js', 'database.js')\n",
    "    \n",
    "    # Convert database to json\n",
    "    convert_taffy2json(inp_fn, out_fn)\n",
    "    \n",
    "    # Add filenames and labels\n",
    "    meta = json.load(open(out_fn))\n",
    "    for m in meta:\n",
    "        file_bn = os.path.splitext(os.path.basename(m['url']))[0]\n",
    "        m['filename'] = '{}/{}'.format(spc, file_bn+'_rawcolor.png')\n",
    "        m['order'] = order[spc]\n",
    "        m['family'] = family[spc]\n",
    "        m['genus'] = genus[spc]\n",
    "        del m['url']\n",
    "        del m['orientation']\n",
    "        del m['clipped_fraction']\n",
    "    \n",
    "    # Index metadata by filename\n",
    "    meta = {m['filename']: m for m in meta}\n",
    "    \n",
    "    # Filter images in db\n",
    "    spc_images = [spc + '/' + fn for fn in os.listdir(os.path.join('rawcolor_db', 'images', spc))]\n",
    "    meta = {fn: meta[fn] for fn in spc_images}\n",
    "    \n",
    "    # Rewrite to metadata\n",
    "    json.dump(meta, open(out_fn, 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "001/132: 20170124_001\n",
      "Train set: 631\n",
      "Valid set: 127\n",
      "Test set: 253\n",
      "\n",
      "002/132: 20170124_002\n",
      "Train set: 501\n",
      "Valid set: 101\n",
      "Test set: 201\n",
      "\n",
      "003/132: 20170126_001\n",
      "Train set: 277\n",
      "Valid set: 55\n",
      "Test set: 111\n",
      "\n",
      "004/132: 20170126_002\n",
      "Train set: 499\n",
      "Valid set: 100\n",
      "Test set: 200\n",
      "\n",
      "005/132: 20170126_003\n",
      "Train set: 500\n",
      "Valid set: 100\n",
      "Test set: 201\n",
      "\n",
      "006/132: 20170130_001\n",
      "Train set: 475\n",
      "Valid set: 95\n",
      "Test set: 191\n",
      "\n",
      "007/132: 20170130_002\n",
      "Train set: 420\n",
      "Valid set: 84\n",
      "Test set: 168\n",
      "\n",
      "008/132: 20170130_003\n",
      "Train set: 497\n",
      "Valid set: 99\n",
      "Test set: 199\n",
      "\n",
      "009/132: 20170131_001\n",
      "Train set: 501\n",
      "Valid set: 100\n",
      "Test set: 201\n",
      "\n",
      "010/132: 20170131_002\n",
      "Train set: 329\n",
      "Valid set: 66\n",
      "Test set: 132\n",
      "\n",
      "011/132: 20170203_001\n",
      "Train set: 460\n",
      "Valid set: 92\n",
      "Test set: 184\n",
      "\n",
      "012/132: 20170203_002\n",
      "Train set: 527\n",
      "Valid set: 105\n",
      "Test set: 211\n",
      "\n",
      "013/132: 20170206_001\n",
      "Train set: 502\n",
      "Valid set: 100\n",
      "Test set: 201\n",
      "\n",
      "014/132: 20170206_002\n",
      "Train set: 569\n",
      "Valid set: 114\n",
      "Test set: 228\n",
      "\n",
      "015/132: 20170207_001\n",
      "Train set: 526\n",
      "Valid set: 106\n",
      "Test set: 211\n",
      "\n",
      "016/132: 20170207_003\n",
      "Train set: 577\n",
      "Valid set: 115\n",
      "Test set: 231\n",
      "\n",
      "017/132: 20170207_004\n",
      "Train set: 454\n",
      "Valid set: 91\n",
      "Test set: 182\n",
      "\n",
      "018/132: 20170207_006\n",
      "Train set: 417\n",
      "Valid set: 83\n",
      "Test set: 167\n",
      "\n",
      "019/132: 20170209_001\n",
      "Train set: 252\n",
      "Valid set: 50\n",
      "Test set: 101\n",
      "\n",
      "020/132: 20170209_002\n",
      "Train set: 250\n",
      "Valid set: 50\n",
      "Test set: 101\n",
      "\n",
      "021/132: 20170209_003\n",
      "Train set: 252\n",
      "Valid set: 50\n",
      "Test set: 101\n",
      "\n",
      "022/132: 20170210_001\n",
      "Train set: 533\n",
      "Valid set: 106\n",
      "Test set: 214\n",
      "\n",
      "023/132: 20170210_002\n",
      "Train set: 359\n",
      "Valid set: 72\n",
      "Test set: 144\n",
      "\n",
      "024/132: 20170213_001\n",
      "Train set: 506\n",
      "Valid set: 101\n",
      "Test set: 203\n",
      "\n",
      "025/132: 20170213_002\n",
      "Train set: 503\n",
      "Valid set: 100\n",
      "Test set: 202\n",
      "\n",
      "026/132: 20170213_003\n",
      "Train set: 504\n",
      "Valid set: 101\n",
      "Test set: 202\n",
      "\n",
      "027/132: 20170214_001\n",
      "Train set: 508\n",
      "Valid set: 101\n",
      "Test set: 204\n",
      "\n",
      "028/132: 20170214_002\n",
      "Train set: 515\n",
      "Valid set: 103\n",
      "Test set: 206\n",
      "\n",
      "029/132: 20170214_003\n",
      "Train set: 557\n",
      "Valid set: 111\n",
      "Test set: 223\n",
      "\n",
      "030/132: 20170214_004\n",
      "Train set: 254\n",
      "Valid set: 51\n",
      "Test set: 102\n",
      "\n",
      "031/132: 20170214_005\n",
      "Train set: 511\n",
      "Valid set: 102\n",
      "Test set: 205\n",
      "\n",
      "032/132: 20170214_006\n",
      "Train set: 263\n",
      "Valid set: 52\n",
      "Test set: 106\n",
      "\n",
      "033/132: 20170216_001\n",
      "Train set: 504\n",
      "Valid set: 101\n",
      "Test set: 202\n",
      "\n",
      "034/132: 20170216_002\n",
      "Train set: 584\n",
      "Valid set: 117\n",
      "Test set: 234\n",
      "\n",
      "035/132: 20170216_003\n",
      "Train set: 504\n",
      "Valid set: 101\n",
      "Test set: 202\n",
      "\n",
      "036/132: 20170216_004\n",
      "Train set: 254\n",
      "Valid set: 51\n",
      "Test set: 102\n",
      "\n",
      "037/132: 20170216_005\n",
      "Train set: 252\n",
      "Valid set: 50\n",
      "Test set: 101\n",
      "\n",
      "038/132: 20170216_006\n",
      "Train set: 254\n",
      "Valid set: 51\n",
      "Test set: 102\n",
      "\n",
      "039/132: 20170217_001\n",
      "Train set: 297\n",
      "Valid set: 59\n",
      "Test set: 119\n",
      "\n",
      "040/132: 20170217_002\n",
      "Train set: 363\n",
      "Valid set: 72\n",
      "Test set: 146\n",
      "\n",
      "041/132: 20170217_003\n",
      "Train set: 265\n",
      "Valid set: 53\n",
      "Test set: 107\n",
      "\n",
      "042/132: 20170217_004\n",
      "Train set: 359\n",
      "Valid set: 72\n",
      "Test set: 144\n",
      "\n",
      "043/132: 20170217_005\n",
      "Train set: 556\n",
      "Valid set: 111\n",
      "Test set: 223\n",
      "\n",
      "044/132: 20170217_006\n",
      "Train set: 256\n",
      "Valid set: 51\n",
      "Test set: 103\n",
      "\n",
      "045/132: 20170217_007\n",
      "Train set: 179\n",
      "Valid set: 36\n",
      "Test set: 72\n",
      "\n",
      "046/132: 20170221_001\n",
      "Train set: 506\n",
      "Valid set: 102\n",
      "Test set: 203\n",
      "\n",
      "047/132: 20170221_002\n",
      "Train set: 504\n",
      "Valid set: 101\n",
      "Test set: 202\n",
      "\n",
      "048/132: 20170221_003\n",
      "Train set: 257\n",
      "Valid set: 51\n",
      "Test set: 103\n",
      "\n",
      "049/132: 20170221_004\n",
      "Train set: 503\n",
      "Valid set: 101\n",
      "Test set: 202\n",
      "\n",
      "050/132: 20170221_005\n",
      "Train set: 251\n",
      "Valid set: 51\n",
      "Test set: 101\n",
      "\n",
      "051/132: 20170221_006\n",
      "Train set: 251\n",
      "Valid set: 50\n",
      "Test set: 101\n",
      "\n",
      "052/132: 20170221_007\n",
      "Train set: 252\n",
      "Valid set: 50\n",
      "Test set: 101\n",
      "\n",
      "053/132: 20170223_001\n",
      "Train set: 277\n",
      "Valid set: 55\n",
      "Test set: 111\n",
      "\n",
      "054/132: 20170223_002\n",
      "Train set: 321\n",
      "Valid set: 65\n",
      "Test set: 129\n",
      "\n",
      "055/132: 20170224_001\n",
      "Train set: 293\n",
      "Valid set: 59\n",
      "Test set: 118\n",
      "\n",
      "056/132: 20170224_002\n",
      "Train set: 532\n",
      "Valid set: 106\n",
      "Test set: 213\n",
      "\n",
      "057/132: 20170224_003\n",
      "Train set: 296\n",
      "Valid set: 60\n",
      "Test set: 119\n",
      "\n",
      "058/132: 20170224_004\n",
      "Train set: 255\n",
      "Valid set: 51\n",
      "Test set: 102\n",
      "\n",
      "059/132: 20170224_005\n",
      "Train set: 224\n",
      "Valid set: 45\n",
      "Test set: 90\n",
      "\n",
      "060/132: 20170224_006\n",
      "Train set: 524\n",
      "Valid set: 105\n",
      "Test set: 210\n",
      "\n",
      "061/132: 20170224_007\n",
      "Train set: 567\n",
      "Valid set: 113\n",
      "Test set: 227\n",
      "\n",
      "062/132: 20170224_008\n",
      "Train set: 522\n",
      "Valid set: 104\n",
      "Test set: 209\n",
      "\n",
      "063/132: 20170224_009\n",
      "Train set: 313\n",
      "Valid set: 62\n",
      "Test set: 126\n",
      "\n",
      "064/132: 20170224_010\n",
      "Train set: 270\n",
      "Valid set: 54\n",
      "Test set: 109\n",
      "\n",
      "065/132: 20170224_011\n",
      "Train set: 253\n",
      "Valid set: 51\n",
      "Test set: 102\n",
      "\n",
      "066/132: 20170224_012\n",
      "Train set: 524\n",
      "Valid set: 105\n",
      "Test set: 210\n",
      "\n",
      "067/132: 20170228_001\n",
      "Train set: 250\n",
      "Valid set: 50\n",
      "Test set: 101\n",
      "\n",
      "068/132: 20170228_002\n",
      "Train set: 377\n",
      "Valid set: 75\n",
      "Test set: 151\n",
      "\n",
      "069/132: 20170228_003\n",
      "Train set: 259\n",
      "Valid set: 52\n",
      "Test set: 104\n",
      "\n",
      "070/132: 20170302_001\n",
      "Train set: 331\n",
      "Valid set: 67\n",
      "Test set: 133\n",
      "\n",
      "071/132: 20170302_002\n",
      "Train set: 266\n",
      "Valid set: 53\n",
      "Test set: 107\n",
      "\n",
      "072/132: 20170303_001\n",
      "Train set: 574\n",
      "Valid set: 115\n",
      "Test set: 230\n",
      "\n",
      "073/132: 20170303_002\n",
      "Train set: 294\n",
      "Valid set: 59\n",
      "Test set: 118\n",
      "\n",
      "074/132: 20170303_003\n",
      "Train set: 542\n",
      "Valid set: 108\n",
      "Test set: 217\n",
      "\n",
      "075/132: 20170303_004\n",
      "Train set: 273\n",
      "Valid set: 55\n",
      "Test set: 110\n",
      "\n",
      "076/132: 20170303_005\n",
      "Train set: 381\n",
      "Valid set: 76\n",
      "Test set: 153\n",
      "\n",
      "077/132: 20170306_001\n",
      "Train set: 343\n",
      "Valid set: 69\n",
      "Test set: 138\n",
      "\n",
      "078/132: 20170306_002\n",
      "Train set: 310\n",
      "Valid set: 62\n",
      "Test set: 125\n",
      "\n",
      "079/132: 20170306_003\n",
      "Train set: 516\n",
      "Valid set: 104\n",
      "Test set: 207\n",
      "\n",
      "080/132: 20170306_004\n",
      "Train set: 552\n",
      "Valid set: 110\n",
      "Test set: 221\n",
      "\n",
      "081/132: 20170306_005\n",
      "Train set: 269\n",
      "Valid set: 54\n",
      "Test set: 108\n",
      "\n",
      "082/132: 20170306_006\n",
      "Train set: 570\n",
      "Valid set: 114\n",
      "Test set: 228\n",
      "\n",
      "083/132: 20170306_007\n",
      "Train set: 254\n",
      "Valid set: 51\n",
      "Test set: 102\n",
      "\n",
      "084/132: 20170307_001\n",
      "Train set: 403\n",
      "Valid set: 80\n",
      "Test set: 162\n",
      "\n",
      "085/132: 20170307_002\n",
      "Train set: 378\n",
      "Valid set: 75\n",
      "Test set: 152\n",
      "\n",
      "086/132: 20170307_003\n",
      "Train set: 282\n",
      "Valid set: 56\n",
      "Test set: 113\n",
      "\n",
      "087/132: 20170420_001\n",
      "Train set: 504\n",
      "Valid set: 101\n",
      "Test set: 202\n",
      "\n",
      "088/132: 20170425_001\n",
      "Train set: 253\n",
      "Valid set: 50\n",
      "Test set: 102\n",
      "\n",
      "089/132: 20170425_002\n",
      "Train set: 129\n",
      "Valid set: 26\n",
      "Test set: 52\n",
      "\n",
      "090/132: 20170425_003\n",
      "Train set: 521\n",
      "Valid set: 104\n",
      "Test set: 209\n",
      "\n",
      "091/132: 20170425_004\n",
      "Train set: 500\n",
      "Valid set: 100\n",
      "Test set: 201\n",
      "\n",
      "092/132: 20170425_005\n",
      "Train set: 466\n",
      "Valid set: 93\n",
      "Test set: 187\n",
      "\n",
      "093/132: 20170427_001\n",
      "Train set: 502\n",
      "Valid set: 100\n",
      "Test set: 201\n",
      "\n",
      "094/132: 20170427_002\n",
      "Train set: 502\n",
      "Valid set: 100\n",
      "Test set: 201\n",
      "\n",
      "095/132: 20170427_003\n",
      "Train set: 496\n",
      "Valid set: 100\n",
      "Test set: 199\n",
      "\n",
      "096/132: 20170427_004\n",
      "Train set: 425\n",
      "Valid set: 85\n",
      "Test set: 171\n",
      "\n",
      "097/132: 20170502_001\n",
      "Train set: 539\n",
      "Valid set: 108\n",
      "Test set: 216\n",
      "\n",
      "098/132: 20170502_002\n",
      "Train set: 310\n",
      "Valid set: 62\n",
      "Test set: 125\n",
      "\n",
      "099/132: 20170504_001\n",
      "Train set: 281\n",
      "Valid set: 56\n",
      "Test set: 113\n",
      "\n",
      "100/132: 20170504_002\n",
      "Train set: 523\n",
      "Valid set: 104\n",
      "Test set: 210\n",
      "\n",
      "101/132: 20170504_003\n",
      "Train set: 498\n",
      "Valid set: 99\n",
      "Test set: 200\n",
      "\n",
      "102/132: 20170504_004\n",
      "Train set: 449\n",
      "Valid set: 90\n",
      "Test set: 180\n",
      "\n",
      "103/132: 20170504_005\n",
      "Train set: 483\n",
      "Valid set: 96\n",
      "Test set: 194\n",
      "\n",
      "104/132: 20170504_006\n",
      "Train set: 502\n",
      "Valid set: 100\n",
      "Test set: 201\n",
      "\n",
      "105/132: 20170504_007\n",
      "Train set: 486\n",
      "Valid set: 98\n",
      "Test set: 195\n",
      "\n",
      "106/132: 20170509_001\n",
      "Train set: 498\n",
      "Valid set: 100\n",
      "Test set: 200\n",
      "\n",
      "107/132: 20170509_002\n",
      "Train set: 502\n",
      "Valid set: 100\n",
      "Test set: 201\n",
      "\n",
      "108/132: 20170509_003\n",
      "Train set: 251\n",
      "Valid set: 51\n",
      "Test set: 101\n",
      "\n",
      "109/132: 20170509_004\n",
      "Train set: 499\n",
      "Valid set: 100\n",
      "Test set: 200\n",
      "\n",
      "110/132: 20170509_005\n",
      "Train set: 372\n",
      "Valid set: 74\n",
      "Test set: 149\n",
      "\n",
      "111/132: 20170516_001\n",
      "Train set: 508\n",
      "Valid set: 102\n",
      "Test set: 204\n",
      "\n",
      "112/132: 20170516_002\n",
      "Train set: 545\n",
      "Valid set: 109\n",
      "Test set: 219\n",
      "\n",
      "113/132: 20170516_003\n",
      "Train set: 502\n",
      "Valid set: 100\n",
      "Test set: 201\n",
      "\n",
      "114/132: 20170516_004\n",
      "Train set: 511\n",
      "Valid set: 103\n",
      "Test set: 205\n",
      "\n",
      "115/132: 20170516_005\n",
      "Train set: 410\n",
      "Valid set: 82\n",
      "Test set: 164\n",
      "\n",
      "116/132: 20170516_006\n",
      "Train set: 497\n",
      "Valid set: 99\n",
      "Test set: 199\n",
      "\n",
      "117/132: 20170516_007\n",
      "Train set: 501\n",
      "Valid set: 101\n",
      "Test set: 201\n",
      "\n",
      "118/132: 20170516_008\n",
      "Train set: 503\n",
      "Valid set: 100\n",
      "Test set: 202\n",
      "\n",
      "119/132: 20170518_001\n",
      "Train set: 500\n",
      "Valid set: 100\n",
      "Test set: 200\n",
      "\n",
      "120/132: 20170601_001\n",
      "Train set: 539\n",
      "Valid set: 108\n",
      "Test set: 216\n",
      "\n",
      "121/132: 20170601_002\n",
      "Train set: 503\n",
      "Valid set: 100\n",
      "Test set: 202\n",
      "\n",
      "122/132: 20170601_003\n",
      "Train set: 506\n",
      "Valid set: 102\n",
      "Test set: 203\n",
      "\n",
      "123/132: 20171128_001\n",
      "Train set: 299\n",
      "Valid set: 60\n",
      "Test set: 120\n",
      "\n",
      "124/132: 20171130_001\n",
      "Train set: 292\n",
      "Valid set: 58\n",
      "Test set: 117\n",
      "\n",
      "125/132: 20171130_004\n",
      "Train set: 625\n",
      "Valid set: 125\n",
      "Test set: 250\n",
      "\n",
      "126/132: 20171205_001\n",
      "Train set: 223\n",
      "Valid set: 45\n",
      "Test set: 90\n",
      "\n",
      "127/132: 20171207_001\n",
      "Train set: 259\n",
      "Valid set: 52\n",
      "Test set: 104\n",
      "\n",
      "128/132: 20171207_002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 313\n",
      "Valid set: 63\n",
      "Test set: 126\n",
      "\n",
      "129/132: 20171207_003\n",
      "Train set: 307\n",
      "Valid set: 61\n",
      "Test set: 123\n",
      "\n",
      "130/132: 20171207_004\n",
      "Train set: 292\n",
      "Valid set: 58\n",
      "Test set: 117\n",
      "\n",
      "131/132: 20171207_005\n",
      "Train set: 307\n",
      "Valid set: 61\n",
      "Test set: 123\n",
      "\n",
      "132/132: 20171207_006\n",
      "Train set: 299\n",
      "Valid set: 60\n",
      "Test set: 120\n"
     ]
    }
   ],
   "source": [
    "OUT_DIR = 'rawcolor_db/subsets'\n",
    "META_DIR = 'rawcolor_db/meta'\n",
    "\n",
    "specimen_list = sorted(os.listdir('rawcolor_db/images'))\n",
    "for i, spc in enumerate(specimen_list):\n",
    "    print '\\n{:03d}/{:03d}: {}'.format(i+1, len(specimen_list), spc)\n",
    "    meta = json.load(open(os.path.join(META_DIR, '{}-meta.json'.format(spc))))\n",
    "    spc_images = meta.keys()\n",
    "    unixtimes = [int(''.join(fn.split('/')[-1].split('-')[1:3])) for fn in spc_images]\n",
    "#     order = np.argsort([float(meta[fn]['timestamp']) for fn in spc_images])\n",
    "    order = np.argsort(unixtimes)\n",
    "    \n",
    "    \n",
    "    num_images = len(meta)\n",
    "    train_set = order[:int(num_images * 0.5)]\n",
    "    valid_set = order[int(num_images * 0.6):int(num_images * 0.7)]\n",
    "    test_set = order[int(num_images * 0.8):]\n",
    "    \n",
    "    open('{}/{}-timeseries.lst'.format(OUT_DIR, spc), 'w').write('\\n'.join([spc_images[o] for o in order]))\n",
    "    open('{}/{}-train.lst'.format(OUT_DIR, spc), 'w').write('\\n'.join([spc_images[o] for o in train_set]))\n",
    "    open('{}/{}-valid.lst'.format(OUT_DIR, spc), 'w').write('\\n'.join([spc_images[o] for o in valid_set]))\n",
    "    open('{}/{}-test.lst'.format(OUT_DIR, spc), 'w').write('\\n'.join([spc_images[o] for o in test_set]))\n",
    "    \n",
    "    print 'Train set: {}\\nValid set: {}\\nTest set: {}'.format(len(train_set), len(valid_set), len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add turk annotation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'rawcolor_db/meta/20170124_001-meta.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-e1e62b8c5a73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mspecimen_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rawcolor_db/images'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspecimen_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mmeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mspc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rawcolor_db/meta/{}-meta.json'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Load turk results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'rawcolor_db/meta/20170124_001-meta.json'"
     ]
    }
   ],
   "source": [
    "# Load all meta files\n",
    "meta = {}\n",
    "specimen_list = sorted(os.listdir('rawcolor_db/images'))\n",
    "for i, spc in enumerate(specimen_list):\n",
    "    meta[spc] = json.load(open('rawcolor_db/meta/{}-meta.json'.format(spc)))\n",
    "\n",
    "# Load turk results\n",
    "results = list(csv.reader(open('turk_results/Batch_3084800_batch_results.csv')))\n",
    "header = results[0]\n",
    "ans_idx = header.index('Answer.annotation_data')\n",
    "wid_idx = header.index('WorkerId')\n",
    "worker_ids = [rst[wid_idx] for rst in results[1:]]\n",
    "results = [rst[ans_idx] for rst in results[1:]]\n",
    "\n",
    "# Add annotations\n",
    "specimen_dict = [l.split() for l in open('turk_results/turk_db_correspondance.lst')]\n",
    "specimen_dict = {spc[0]: spc[1] for spc in specimen_dict}\n",
    "for it, (wid, rst) in enumerate(zip(worker_ids, results)):\n",
    "    if it % 100 == 0:\n",
    "        print it, 'of', len(worker_ids), 'done!'\n",
    "        \n",
    "    hit = json.loads(rst)\n",
    "    for entry in hit:\n",
    "        # Read image and specimen label\n",
    "        lbl = entry['url'].split('/')[-2]\n",
    "        spc = specimen_dict[lbl].replace('/', '_')\n",
    "        if spc not in specimen_list:\n",
    "            continue\n",
    "            \n",
    "        ann_image = os.path.join(spc, entry['url'].split('/')[-1][:-4]+'_rawcolor.png')\n",
    "        if not os.path.isfile(os.path.join('rawcolor_db', 'images', ann_image)):\n",
    "            continue\n",
    "        img = sio.imread(os.path.join('rawcolor_db', 'images', ann_image))\n",
    "        scale = float(img.shape[1]) / 200.\n",
    "        \n",
    "        # Prepare turk annotations\n",
    "        annotation = {key: entry[key] for key in ['focus', 'z-dir', 'confidence']}\n",
    "        annotation['worker_id'] = wid\n",
    "        for tag in ['head', 'tail']:\n",
    "            if isinstance(entry[tag], dict):\n",
    "                annotation[tag] = {coord: val * scale for coord, val in entry[tag].iteritems()}\n",
    "            else:\n",
    "                annotation[tag] = entry[tag]\n",
    "                \n",
    "        # Add to metadata\n",
    "        if 'annotation' not in meta[spc][ann_image]:\n",
    "            meta[spc][ann_image]['annotation'] = []\n",
    "        meta[spc][ann_image]['annotation'].append(annotation)\n",
    "        \n",
    "#         plt.figure()\n",
    "#         plt.imshow(img)\n",
    "#         head, tail = annotation['head'], annotation['tail']\n",
    "#         plt.gca().arrow(tail['x'], tail['y'], (head['x'] - tail['x']), (head['y'] - tail['y']), fc='b', ec='b', head_width=5, head_length=10)\n",
    "#         sdkvcb\n",
    "\n",
    "# Save metadata\n",
    "for spc in meta:\n",
    "    out_fn = 'rawcolor_db/meta/{}-meta.json'.format(spc)\n",
    "    json.dump(meta[spc], open(out_fn, 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
