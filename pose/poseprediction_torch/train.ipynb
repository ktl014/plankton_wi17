{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-25T10:15:06.933862Z",
     "start_time": "2018-02-25T10:15:03.214632Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from utils.data import *\n",
    "from transform import *\n",
    "from dataset import PlanktonDataset\n",
    "from model import PoseModel\n",
    "from visualdl import LogWriter\n",
    "from utils.vis import show_arrow, show_arrow_batch\n",
    "from utils.data import eval_euc_dists\n",
    "import shutil\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-25T10:15:06.948533Z",
     "start_time": "2018-02-25T10:15:06.939051Z"
    }
   },
   "outputs": [],
   "source": [
    "log_dir = \"./log\"\n",
    "img_dir = '/data5/Plankton_wi18/rawcolor_db/images'\n",
    "csv_filename = 'data/data_{}.csv'\n",
    "\n",
    "phases = ['train', 'valid', 'test']\n",
    "\n",
    "# dataset_mean, dataset_std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "normalize = Normalize([0.5, 0.5, 0.5], [1, 1, 1])\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "input_size = (384, 384)\n",
    "\n",
    "_GPU = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-25T10:15:07.500873Z",
     "start_time": "2018-02-25T10:15:06.952791Z"
    }
   },
   "outputs": [],
   "source": [
    "data_transform = {\n",
    "    'train': transforms.Compose([\n",
    "        Rescale(input_size),\n",
    "        RandomHorizontalFlip(),\n",
    "        RandomVerticalFlip(),\n",
    "        ToTensor(),\n",
    "        normalize\n",
    "    ]),\n",
    "    'valid': transforms.Compose([\n",
    "        Rescale(input_size),\n",
    "        ToTensor(),\n",
    "        normalize\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        Rescale(input_size),\n",
    "        ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "}\n",
    "\n",
    "datasets = {x: PlanktonDataset(csv_file=csv_filename.format(x),\n",
    "                               img_dir=img_dir,\n",
    "                               transform=data_transform[x])\n",
    "            for x in phases}\n",
    "\n",
    "dataloaders = {x: DataLoader(datasets[x], batch_size=batch_size,\n",
    "                             shuffle=True, num_workers=4)\n",
    "               for x in phases}\n",
    "\n",
    "dataset_sizes = {x: len(datasets[x]) for x in phases}\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "n_per_epoch = int(np.ceil(len(datasets['train']) / batch_size))\n",
    "\n",
    "losses, errs = {}, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-25T10:15:07.516544Z",
     "start_time": "2018-02-25T10:15:07.507405Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, filename='model_checkpoints/checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_checkpoints/model_best.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-25T10:15:07.665926Z",
     "start_time": "2018-02-25T10:15:07.522316Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, logger, num_epoch=25):\n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_err = float('inf')\n",
    "    is_best = False\n",
    "    \n",
    "    for phase in ['train', 'valid']:\n",
    "        try:\n",
    "            with logger.mode(phase):\n",
    "                if phase not in losses:\n",
    "                    losses[phase] = logger.scalar('scalars/{}_loss'.format(phase))\n",
    "                if phase not in errs:\n",
    "                    errs[phase] = logger.scalar('scalars/{}_err'.format(phase))\n",
    "            cnt = 0\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    for epoch in range(num_epoch):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epoch))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train(True)\n",
    "            else:\n",
    "                model.train(False)\n",
    "                \n",
    "            running_loss = 0.0\n",
    "            running_err = 0.0\n",
    "            \n",
    "            epoch_since = time.time()\n",
    "            total = 0\n",
    "            \n",
    "            for i, data in enumerate(dataloaders[phase]):\n",
    "                inputs, target, coordinates = data['image'], data['target_map'], data['coordinates']\n",
    "                \n",
    "                if use_gpu:\n",
    "                    inputs = Variable(inputs.cuda(_GPU))\n",
    "                    target = Variable(target.cuda(_GPU))\n",
    "                else:\n",
    "                    inputs, target = Variable(inputs), Variable(target)\n",
    "                    \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, target)\n",
    "                err = eval_euc_dists(outputs.cpu().data.numpy(), coordinates.numpy())\n",
    "                \n",
    "                running_loss += loss.data[0] * inputs.size(0)\n",
    "                running_err += err['average'] * inputs.size(0)\n",
    "                \n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    losses[phase].add_record(cnt, loss.data[0])\n",
    "                    errs[phase].add_record(cnt, err['average'])\n",
    "                    cnt += 1\n",
    "                    \n",
    "                eta = (time.time() - epoch_since) / total * len(datasets[phase])\n",
    "                total += inputs.size(0)\n",
    "\n",
    "                print('{} {}/{} ({:.0f}%), Loss: {:.4f}, Error: {:.4f}, ETA: {:.0f}s     \\r'\n",
    "                      .format('Training' if phase == 'train' else 'validating', \n",
    "                              total, len(datasets[phase]), 100.0 * total / len(datasets[phase]), \n",
    "                              running_loss / total, running_err / total, eta), end='')\n",
    "            \n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_err = running_err / dataset_sizes[phase]\n",
    "            \n",
    "            if phase == 'valid':\n",
    "                losses[phase].add_record(epoch, epoch_loss)\n",
    "                errs[phase].add_record(epoch, epoch_err)\n",
    "                if epoch_err < best_err:\n",
    "                    is_best = True\n",
    "                    best_err = epoch_err\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            \n",
    "            print()\n",
    "            print('{} Loss: {:.4f} Error: {:.4f} Time Elapsed: {:.0f}s'\n",
    "                  .format(phase, epoch_loss, epoch_err, time.time() - epoch_since))\n",
    "            print()\n",
    "        \n",
    "        save_checkpoint({\n",
    "            'epoch': epoch,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_err': best_err,\n",
    "            'optimizer' : optimizer.state_dict(),\n",
    "            'cnt': cnt\n",
    "        }, is_best)\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}, {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val error: {:4f}'.format(best_err))\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-25T10:15:26.049996Z",
     "start_time": "2018-02-25T10:15:07.669870Z"
    }
   },
   "outputs": [],
   "source": [
    "model = PoseModel().cuda(_GPU)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer = optim.Adam(model.parameters(), weight_decay=0.0005)\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "logger = LogWriter(log_dir, sync_cycle=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-02-25T10:15:27.714Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/15\n",
      "----------\n",
      "Training 44328/44328 (100%), Loss: 2.3800, Error: 256.0594, ETA: 0s        \n",
      "train Loss: 2.3800 Error: 256.0594 Time Elapsed: 2565s\n",
      "\n",
      "validating 8844/8844 (100%), Loss: 0.0125, Error: 185.1612, ETA: 600s     \n",
      "valid Loss: 0.0125 Error: 185.1612 Time Elapsed: 150s\n",
      "\n",
      "\n",
      "Epoch 1/15\n",
      "----------\n",
      "Training 44328/44328 (100%), Loss: 0.0126, Error: 182.3008, ETA: 0s       \n",
      "train Loss: 0.0126 Error: 182.3008 Time Elapsed: 2611s\n",
      "\n",
      "validating 8844/8844 (100%), Loss: 0.0124, Error: 180.4375, ETA: 581s     \n",
      "valid Loss: 0.0124 Error: 180.4375 Time Elapsed: 145s\n",
      "\n",
      "\n",
      "Epoch 2/15\n",
      "----------\n",
      "Training 44328/44328 (100%), Loss: 0.0125, Error: 181.8024, ETA: 0s       \n",
      "train Loss: 0.0125 Error: 181.8024 Time Elapsed: 2626s\n",
      "\n",
      "validating 8844/8844 (100%), Loss: 0.0126, Error: 178.6900, ETA: 556s     \n",
      "valid Loss: 0.0126 Error: 178.6900 Time Elapsed: 139s\n",
      "\n",
      "\n",
      "Epoch 3/15\n",
      "----------\n",
      "Training 44328/44328 (100%), Loss: 0.0125, Error: 181.0284, ETA: 0s       \n",
      "train Loss: 0.0125 Error: 181.0284 Time Elapsed: 2673s\n",
      "\n",
      "validating 8844/8844 (100%), Loss: 0.0123, Error: 177.7073, ETA: 597s     \n",
      "valid Loss: 0.0123 Error: 177.7073 Time Elapsed: 149s\n",
      "\n",
      "\n",
      "Epoch 4/15\n",
      "----------\n",
      "Training 44328/44328 (100%), Loss: 269450594.2245, Error: 895.0902, ETA: 0s       \n",
      "train Loss: 269450594.2245 Error: 895.0902 Time Elapsed: 2553s\n",
      "\n",
      "validating 8844/8844 (100%), Loss: 0.0152, Error: 1056.8342, ETA: 550s     \n",
      "valid Loss: 0.0152 Error: 1056.8342 Time Elapsed: 137s\n",
      "\n",
      "\n",
      "Epoch 5/15\n",
      "----------\n",
      "Training 44328/44328 (100%), Loss: 2975.7204, Error: 1110.8540, ETA: 0s      \n",
      "train Loss: 2975.7204 Error: 1110.8540 Time Elapsed: 2532s\n",
      "\n",
      "validating 8844/8844 (100%), Loss: 0.0151, Error: 1056.8342, ETA: 631s     \n",
      "valid Loss: 0.0151 Error: 1056.8342 Time Elapsed: 157s\n",
      "\n",
      "\n",
      "Epoch 6/15\n",
      "----------\n",
      "Training 44328/44328 (100%), Loss: 561011.7772, Error: 1118.6837, ETA: 0s      \n",
      "train Loss: 561011.7772 Error: 1118.6837 Time Elapsed: 2562s\n",
      "\n",
      "validating 8844/8844 (100%), Loss: 0.0157, Error: 1099.3972, ETA: 624s     \n",
      "valid Loss: 0.0157 Error: 1099.3972 Time Elapsed: 156s\n",
      "\n",
      "\n",
      "Epoch 7/15\n",
      "----------\n",
      "Training 44328/44328 (100%), Loss: 0.0156, Error: 1149.2243, ETA: 0s       \n",
      "train Loss: 0.0156 Error: 1149.2243 Time Elapsed: 2556s\n",
      "\n",
      "validating 8844/8844 (100%), Loss: 0.0155, Error: 1115.4825, ETA: 531s     \n",
      "valid Loss: 0.0155 Error: 1115.4825 Time Elapsed: 133s\n",
      "\n",
      "\n",
      "Epoch 8/15\n",
      "----------\n",
      "Training 44328/44328 (100%), Loss: 0.7337, Error: 1110.0376, ETA: 0s       \n",
      "train Loss: 0.7337 Error: 1110.0376 Time Elapsed: 2584s\n",
      "\n",
      "validating 8844/8844 (100%), Loss: 0.0152, Error: 1056.8342, ETA: 543s     \n",
      "valid Loss: 0.0152 Error: 1056.8342 Time Elapsed: 136s\n",
      "\n",
      "\n",
      "Epoch 9/15\n",
      "----------\n",
      "Training 44328/44328 (100%), Loss: 194.5128, Error: 1107.0265, ETA: 0s      \n",
      "train Loss: 194.5128 Error: 1107.0265 Time Elapsed: 2694s\n",
      "\n",
      "validating 8844/8844 (100%), Loss: 0.0152, Error: 1056.8342, ETA: 586s     \n",
      "valid Loss: 0.0152 Error: 1056.8342 Time Elapsed: 146s\n",
      "\n",
      "\n",
      "Epoch 10/15\n",
      "----------\n",
      "Training 44328/44328 (100%), Loss: 47.8620, Error: 1108.7635, ETA: 0s       \n",
      "train Loss: 47.8620 Error: 1108.7635 Time Elapsed: 2754s\n",
      "\n",
      "validating 8844/8844 (100%), Loss: 0.1351, Error: 1250.9882, ETA: 580s     \n",
      "valid Loss: 0.1351 Error: 1250.9882 Time Elapsed: 145s\n",
      "\n",
      "\n",
      "Epoch 11/15\n",
      "----------\n",
      "Training 44328/44328 (100%), Loss: 0.6357, Error: 1126.7970, ETA: 0s       \n",
      "train Loss: 0.6357 Error: 1126.7970 Time Elapsed: 2780s\n",
      "\n",
      "validating 8844/8844 (100%), Loss: 0.0152, Error: 1124.5537, ETA: 585s     \n",
      "valid Loss: 0.0152 Error: 1124.5537 Time Elapsed: 146s\n",
      "\n",
      "\n",
      "Epoch 12/15\n",
      "----------\n",
      "Training 44328/44328 (100%), Loss: 0.0355, Error: 1107.3186, ETA: 0s       \n",
      "train Loss: 0.0355 Error: 1107.3186 Time Elapsed: 2844s\n",
      "\n",
      "validating 8844/8844 (100%), Loss: 0.0151, Error: 1056.8342, ETA: 638s     \n",
      "valid Loss: 0.0151 Error: 1056.8342 Time Elapsed: 159s\n",
      "\n",
      "\n",
      "Epoch 13/15\n",
      "----------\n",
      "Training 34160/44328 (77%), Loss: 3.6284, Error: 1106.3039, ETA: 651s      \r"
     ]
    }
   ],
   "source": [
    "model = train_model(model, criterion, optimizer, exp_lr_scheduler, logger, num_epoch=15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 284,
   "position": {
    "height": "851px",
    "left": "1539px",
    "right": "20px",
    "top": "125px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
