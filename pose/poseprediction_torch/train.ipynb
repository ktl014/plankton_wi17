{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-25T00:43:23.526766Z",
     "start_time": "2018-02-25T00:43:20.001208Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from utils.data import *\n",
    "from transform import *\n",
    "from dataset import PlanktonDataset\n",
    "from model import PoseModel\n",
    "from visualdl import LogWriter\n",
    "from utils.vis import show_arrow\n",
    "from utils.data import eval_euc_dists\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-24T09:16:22.530521Z",
     "start_time": "2018-02-24T09:16:22.523051Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_dir = \"./log\"\n",
    "img_dir = '/data5/Plankton_wi18/rawcolor_db/images'\n",
    "csv_filename = 'data/data_{}.csv'\n",
    "phases = ['train', 'valid', 'test']\n",
    "batch_size = 32\n",
    "_GPU = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-24T09:16:23.845686Z",
     "start_time": "2018-02-24T09:16:23.226791Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_transform = {\n",
    "    'train': transforms.Compose([\n",
    "        Rescale((224, 224)),\n",
    "        RandomHorizontalFlip(),\n",
    "        RandomVerticalFlip(),\n",
    "        ToTensor(),\n",
    "        Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'valid': transforms.Compose([\n",
    "        Rescale((224, 224)),\n",
    "        ToTensor(),\n",
    "        Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        Rescale((224, 224)),\n",
    "        ToTensor(),\n",
    "        Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n",
    "datasets = {x: PlanktonDataset(csv_file=csv_filename.format(x),\n",
    "                               img_dir=img_dir,\n",
    "                               transform=data_transform[x])\n",
    "            for x in phases}\n",
    "\n",
    "dataloaders = {x: DataLoader(datasets[x], batch_size=batch_size,\n",
    "                             shuffle=True, num_workers=4)\n",
    "               for x in phases}\n",
    "\n",
    "dataset_sizes = {x: len(datasets[x]) for x in phases}\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "n_per_epoch = int(np.ceil(len(datasets['train']) / batch_size))\n",
    "\n",
    "losses, errs = {}, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_best.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-24T09:16:24.752894Z",
     "start_time": "2018-02-24T09:16:24.638619Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, logger, num_epoch=25):\n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    for phase in ['train', 'valid']:\n",
    "        try:\n",
    "            with logger.mode(phase):\n",
    "                if phase not in losses:\n",
    "                    losses[phase] = logger.scalar('scalars/{}_loss'.format(phase))\n",
    "                if phase not in errs:\n",
    "                    errs[phase] = logger.scalar('scalars/{}_err'.format(phase))\n",
    "            cnt = 0\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    for epoch in range(num_epoch):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epoch))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train(True)\n",
    "            else:\n",
    "                model.train(False)\n",
    "                \n",
    "            running_loss = 0.0\n",
    "            running_err = 0.0\n",
    "            \n",
    "            for data in dataloaders[phase]:\n",
    "                inputs, target, coordinates = data['image'], data['target_map'], data['coordinates']\n",
    "                \n",
    "                if use_gpu:\n",
    "                    inputs = Variable(inputs.cuda(_GPU))\n",
    "                    target = Variable(target.cuda(_GPU))\n",
    "                else:\n",
    "                    inputs, target = Variable(inputs), Variable(target)\n",
    "                    \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, target)\n",
    "                err = eval_euc_dists(outputs.cpu().data.numpy(), coordinates.numpy())\n",
    "                \n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    losses[phase].add_record(cnt, loss.data[0])\n",
    "                    errs[phase].add_record(cnt, err['average'])\n",
    "                    cnt += 1\n",
    "                    \n",
    "                running_loss += loss.data[0] * inputs.size(0)\n",
    "                running_err += err['average'] * inputs.size(0)\n",
    "            \n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_err = running_err / dataset_sizes[phase]\n",
    "            \n",
    "            if phase == 'valid':\n",
    "                losses[phase].add_record(epoch, epoch_loss)\n",
    "                errs[phase].add_record(epoch, epoch_err)\n",
    "            \n",
    "            print('{} Loss: {:.4f}'. format(phase, epoch_loss))\n",
    "            \n",
    "            if phase == 'valid' and epoch_loss > best_loss:\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        \n",
    "        save_checkpoint({\n",
    "            'epoch': epoch,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_prec1': best_prec1,\n",
    "            'optimizer' : optimizer.state_dict(),\n",
    "        }, is_best)\n",
    "        \n",
    "        print()\n",
    "            \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:0f}, {:0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val loss: {:4f}'.format(best_loss))\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-24T09:16:45.423598Z",
     "start_time": "2018-02-24T09:16:28.354993Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = PoseModel().cuda(_GPU)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "logger = LogWriter(log_dir, sync_cycle=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-24T13:06:43.351617Z",
     "start_time": "2018-02-24T09:16:46.509776Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/25\n",
      "----------\n",
      "train Loss: 0.0384\n",
      "valid Loss: 0.0225\n",
      "\n",
      "Epoch 1/25\n",
      "----------\n",
      "train Loss: 0.0213\n",
      "valid Loss: 0.0207\n",
      "\n",
      "Epoch 2/25\n",
      "----------\n",
      "train Loss: 0.0201\n",
      "valid Loss: 0.0199\n",
      "\n",
      "Epoch 3/25\n",
      "----------\n",
      "train Loss: 0.0193\n",
      "valid Loss: 0.0192\n",
      "\n",
      "Epoch 4/25\n",
      "----------\n",
      "train Loss: 0.0188\n",
      "valid Loss: 0.0188\n",
      "\n",
      "Epoch 5/25\n",
      "----------\n",
      "train Loss: 0.0182\n",
      "valid Loss: 0.0183\n",
      "\n",
      "Epoch 6/25\n",
      "----------\n",
      "train Loss: 0.0177\n",
      "valid Loss: 0.0178\n",
      "\n",
      "Epoch 7/25\n",
      "----------\n",
      "train Loss: 0.0174\n",
      "valid Loss: 0.0177\n",
      "\n",
      "Epoch 8/25\n",
      "----------\n",
      "train Loss: 0.0174\n",
      "valid Loss: 0.0177\n",
      "\n",
      "Epoch 9/25\n",
      "----------\n",
      "train Loss: 0.0173\n",
      "valid Loss: 0.0176\n",
      "\n",
      "Epoch 10/25\n",
      "----------\n",
      "train Loss: 0.0173\n",
      "valid Loss: 0.0176\n",
      "\n",
      "Epoch 11/25\n",
      "----------\n",
      "train Loss: 0.0172\n",
      "valid Loss: 0.0176\n",
      "\n",
      "Epoch 12/25\n",
      "----------\n",
      "train Loss: 0.0172\n",
      "valid Loss: 0.0175\n",
      "\n",
      "Epoch 13/25\n",
      "----------\n",
      "train Loss: 0.0171\n",
      "valid Loss: 0.0175\n",
      "\n",
      "Epoch 14/25\n",
      "----------\n",
      "train Loss: 0.0171\n",
      "valid Loss: 0.0174\n",
      "\n",
      "Epoch 15/25\n",
      "----------\n",
      "train Loss: 0.0171\n",
      "valid Loss: 0.0174\n",
      "\n",
      "Epoch 16/25\n",
      "----------\n",
      "train Loss: 0.0171\n",
      "valid Loss: 0.0174\n",
      "\n",
      "Epoch 17/25\n",
      "----------\n",
      "train Loss: 0.0171\n",
      "valid Loss: 0.0174\n",
      "\n",
      "Epoch 18/25\n",
      "----------\n",
      "train Loss: 0.0171\n",
      "valid Loss: 0.0174\n",
      "\n",
      "Epoch 19/25\n",
      "----------\n",
      "train Loss: 0.0171\n",
      "valid Loss: 0.0174\n",
      "\n",
      "Epoch 20/25\n",
      "----------\n",
      "train Loss: 0.0171\n",
      "valid Loss: 0.0174\n",
      "\n",
      "Epoch 21/25\n",
      "----------\n",
      "train Loss: 0.0171\n",
      "valid Loss: 0.0174\n",
      "\n",
      "Epoch 22/25\n",
      "----------\n",
      "train Loss: 0.0171\n",
      "valid Loss: 0.0174\n",
      "\n",
      "Epoch 23/25\n",
      "----------\n",
      "train Loss: 0.0171\n",
      "valid Loss: 0.0174\n",
      "\n",
      "Epoch 24/25\n",
      "----------\n",
      "train Loss: 0.0171\n",
      "valid Loss: 0.0174\n",
      "\n",
      "Training complete in 229.000000, 56.813260s\n",
      "Best val loss:  inf\n"
     ]
    }
   ],
   "source": [
    "model = train_model(model, criterion, optimizer, exp_lr_scheduler, logger, num_epoch=25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 284,
   "position": {
    "height": "851px",
    "left": "1539px",
    "right": "20px",
    "top": "125px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
